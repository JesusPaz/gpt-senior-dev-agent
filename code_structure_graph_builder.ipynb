{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8a1721-02df-4e79-b9b7-083bc43afa30",
   "metadata": {},
   "source": [
    "# 🧠 GPT now understands my repo like a senior dev – here's how\n",
    "\n",
    "This notebook is an end-to-end reproduction and reinterpretation of the **CodeRAG** framework (see [paper](https://arxiv.org/pdf/2504.10046)), adapted to run locally on your own codebase.\n",
    "\n",
    "We want to give GPT (or any LLM) the ability to:\n",
    "- Parse and **understand your entire repo**\n",
    "- Retrieve and reason over related code\n",
    "- Answer questions like a senior developer would — with **zero fine-tuning**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Goal of this experiment\n",
    "\n",
    "Our objective is to build a **code-aware assistant** using a combination of:\n",
    "\n",
    "- 🔍 **Tree-sitter** to parse and structure the codebase\n",
    "- 🧠 **LLMs** to describe each function or class (aka \"requirements\")\n",
    "- 🕸️ **Code graphs** to represent dependencies (calls, imports, etc)\n",
    "- 🧭 **Agentic reasoning** to let the LLM query and retrieve context dynamically\n",
    "- ⚡ **RAG (Retrieval-Augmented Generation)** to reduce hallucinations and give smarter answers\n",
    "\n",
    "The end result is a local-first, fully transparent, and extensible RAG pipeline tailored for your own project.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Inspired by CodeRAG (What we're replicating)\n",
    "\n",
    "From the CodeRAG paper (April 2025), we aim to recreate the following innovations:\n",
    "\n",
    "1. **Requirement Graph**  \n",
    "   A graph where each node is a *natural language description* of a function or class. Edges represent semantic similarity or parent-child relations.\n",
    "\n",
    "2. **DS-Code Graph**  \n",
    "   A code graph that encodes structural dependencies like:\n",
    "   - function calls\n",
    "   - class inheritance\n",
    "   - file/module containment\n",
    "   - semantic similarity (via embeddings)\n",
    "\n",
    "3. **BiGraph Mapping**  \n",
    "   Links between requirements and code elements — allowing retrieval of relevant code given a high-level prompt.\n",
    "\n",
    "4. **Agentic Reasoning**  \n",
    "   An LLM-driven reasoning loop that dynamically:\n",
    "   - queries the graph\n",
    "   - follows dependencies\n",
    "   - does web search if needed\n",
    "   - formats and tests generated code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbbf10-7219-41aa-ab35-ae06a06fb5f8",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5baed20f-b420-4afd-87a2-819ed4bf6c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-sitter-language-pack in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: sentence_transformers in /opt/anaconda3/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: pyvis in /opt/anaconda3/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.86.0)\n",
      "Requirement already satisfied: duckduckgo-search in /opt/anaconda3/lib/python3.12/site-packages (8.0.3)\n",
      "Requirement already satisfied: black in /opt/anaconda3/lib/python3.12/site-packages (24.8.0)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (0.3.22)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: langgraph in /opt/anaconda3/lib/python3.12/site-packages (0.4.8)\n",
      "Requirement already satisfied: tree-sitter>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from tree-sitter-language-pack) (0.24.0)\n",
      "Requirement already satisfied: tree-sitter-c-sharp>=0.23.1 in /opt/anaconda3/lib/python3.12/site-packages (from tree-sitter-language-pack) (0.23.1)\n",
      "Requirement already satisfied: tree-sitter-embedded-template>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from tree-sitter-language-pack) (0.23.2)\n",
      "Requirement already satisfied: tree-sitter-yaml>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tree-sitter-language-pack) (0.7.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (2.7.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (0.32.4)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyvis) (8.27.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyvis) (3.1.4)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from pyvis) (4.1.1)\n",
      "Requirement already satisfied: networkx>=1.11 in /opt/anaconda3/lib/python3.12/site-packages (from pyvis) (3.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.65)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /opt/anaconda3/lib/python3.12/site-packages (from duckduckgo-search) (8.2.1)\n",
      "Requirement already satisfied: primp>=0.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from duckduckgo-search) (0.15.0)\n",
      "Requirement already satisfied: lxml>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from duckduckgo-search) (5.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from black) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/anaconda3/lib/python3.12/site-packages (from black) (24.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from black) (0.10.3)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/anaconda3/lib/python3.12/site-packages (from black) (3.10.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (2.0.26)\n",
      "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (0.2.2)\n",
      "Requirement already satisfied: langgraph-sdk>=0.1.42 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (0.1.70)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.3)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2>=2.9.6->pyvis) (2.1.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from asttokens->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tree-sitter-language-pack sentence_transformers scikit-learn pyvis langchain openai duckduckgo-search black langchain-community langchain-openai python-dotenv langgraph \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1934e129-9a69-4bd9-bbea-91e73a1261f7",
   "metadata": {},
   "source": [
    "## Parse all Python files in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8539ed5f-4257-4ef8-8fa5-fe03eb07acae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved 12 elements with docstrings\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>docstring</th>\n",
       "      <th>start_line</th>\n",
       "      <th>end_line</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_definition</td>\n",
       "      <td>root</td>\n",
       "      <td></td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>main.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>function_definition</td>\n",
       "      <td>get_token_header</td>\n",
       "      <td>Dependency to validate X-Token header</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>dependencies.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>function_definition</td>\n",
       "      <td>get_query_token</td>\n",
       "      <td>Dependency to validate token query parameter</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>dependencies.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class_definition</td>\n",
       "      <td>Task</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>routers/tasks.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>function_definition</td>\n",
       "      <td>get_tasks</td>\n",
       "      <td>Get all tasks with optional status filter</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>routers/tasks.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  type              name  \\\n",
       "0  function_definition              root   \n",
       "1  function_definition  get_token_header   \n",
       "2  function_definition   get_query_token   \n",
       "3     class_definition              Task   \n",
       "4  function_definition         get_tasks   \n",
       "\n",
       "                                      docstring  start_line  end_line  \\\n",
       "0                                                        18        19   \n",
       "1         Dependency to validate X-Token header           5        11   \n",
       "2  Dependency to validate token query parameter          14        20   \n",
       "3                                                         7        12   \n",
       "4     Get all tasks with optional status filter          33        39   \n",
       "\n",
       "               file  \n",
       "0           main.py  \n",
       "1   dependencies.py  \n",
       "2   dependencies.py  \n",
       "3  routers/tasks.py  \n",
       "4  routers/tasks.py  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast, json, pandas as pd\n",
    "from tree_sitter_language_pack import get_parser\n",
    "from pathlib import Path\n",
    "\n",
    "parser = get_parser(\"python\")\n",
    "ROOT   = Path(\"./app\").resolve()\n",
    "out    = []\n",
    "\n",
    "def ts_name_and_span(src: str):\n",
    "    \"\"\"Return dict {name,start,end} for every function/class via Tree-sitter.\"\"\"\n",
    "    tree = parser.parse(src.encode())\n",
    "    root = tree.root_node\n",
    "    res  = []\n",
    "\n",
    "    def walk(node):\n",
    "        if node.type in (\"function_definition\", \"class_definition\"):\n",
    "            name = node.child_by_field_name(\"name\").text.decode()\n",
    "            res.append((name, node.start_point[0]+1, node.end_point[0]+1))\n",
    "        for c in node.named_children:\n",
    "            walk(c)\n",
    "    walk(root)\n",
    "    return res\n",
    "\n",
    "for py in ROOT.rglob(\"*.py\"):\n",
    "    code = py.read_text(encoding=\"utf-8\")\n",
    "    # 1️⃣ positions with tree-sitter\n",
    "    spans = ts_name_and_span(code)\n",
    "    # 2️⃣ docstrings with ast\n",
    "    module = ast.parse(code, filename=str(py))\n",
    "    for node in ast.walk(module):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "            doc = ast.get_docstring(node) or \"\"\n",
    "            name = node.name\n",
    "            # match span (name is unique inside file)\n",
    "            start, end = next((s,e) for n,s,e in spans if n == name)\n",
    "            out.append({\n",
    "                \"type\": \"class_definition\" if isinstance(node, ast.ClassDef) else \"function_definition\",\n",
    "                \"name\": name,\n",
    "                \"docstring\": doc,\n",
    "                \"start_line\": start,\n",
    "                \"end_line\": end,\n",
    "                \"file\": str(py.relative_to(ROOT))\n",
    "            })\n",
    "\n",
    "with open(\"code_elements_with_docstrings.json\", \"w\") as f:\n",
    "    json.dump(out, f, indent=2)\n",
    "\n",
    "print(\"✅ saved\", len(out), \"elements with docstrings\")\n",
    "pd.DataFrame(out).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828125fc-230b-441e-9f10-29b7d1a57f08",
   "metadata": {},
   "source": [
    "## Load elements (docstrings included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c53aa91-240f-4498-838b-f6a9ccf42426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>docstring</th>\n",
       "      <th>start_line</th>\n",
       "      <th>end_line</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_definition</td>\n",
       "      <td>root</td>\n",
       "      <td></td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>main.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>function_definition</td>\n",
       "      <td>get_token_header</td>\n",
       "      <td>Dependency to validate X-Token header</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>dependencies.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>function_definition</td>\n",
       "      <td>get_query_token</td>\n",
       "      <td>Dependency to validate token query parameter</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>dependencies.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class_definition</td>\n",
       "      <td>Task</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>routers/tasks.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>function_definition</td>\n",
       "      <td>get_tasks</td>\n",
       "      <td>Get all tasks with optional status filter</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>routers/tasks.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  type              name  \\\n",
       "0  function_definition              root   \n",
       "1  function_definition  get_token_header   \n",
       "2  function_definition   get_query_token   \n",
       "3     class_definition              Task   \n",
       "4  function_definition         get_tasks   \n",
       "\n",
       "                                      docstring  start_line  end_line  \\\n",
       "0                                                        18        19   \n",
       "1         Dependency to validate X-Token header           5        11   \n",
       "2  Dependency to validate token query parameter          14        20   \n",
       "3                                                         7        12   \n",
       "4     Get all tasks with optional status filter          33        39   \n",
       "\n",
       "               file  \n",
       "0           main.py  \n",
       "1   dependencies.py  \n",
       "2   dependencies.py  \n",
       "3  routers/tasks.py  \n",
       "4  routers/tasks.py  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, pandas as pd, networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "with open(\"code_elements_with_docstrings.json\") as f:\n",
    "    elements = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(elements)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268632cf-9ac0-4613-9261-48c488f5754e",
   "metadata": {},
   "source": [
    "## Requirement Graph – build *similar_to* edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4caa1e7e-9f40-4807-b90e-6491362098e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = [e[\"docstring\"] or \"\" for e in elements]\n",
    "emb = model.encode(texts, normalize_embeddings=True)\n",
    "\n",
    "RG = nx.Graph()\n",
    "\n",
    "# add nodes\n",
    "for idx, e in enumerate(elements):\n",
    "    RG.add_node(f\"R{idx}\", **e)\n",
    "\n",
    "# similarity edges\n",
    "cos_matrix = cosine_similarity(emb)\n",
    "THRESH = 0.8\n",
    "for i in range(len(elements)):\n",
    "    for j in range(i + 1, len(elements)):\n",
    "        if cos_matrix[i, j] >= THRESH:\n",
    "            RG.add_edge(f\"R{i}\", f\"R{j}\", kind=\"similar_to\", weight=float(cos_matrix[i, j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b684876b-9c71-435a-991e-34ad75792063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RG: 12 nodes  |  3 similar_to edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"RG: {RG.number_of_nodes():,} nodes  |  {RG.number_of_edges():,} similar_to edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeae032b-22d9-4577-a454-758d5f48da25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 12\n",
      "Edges: 3\n",
      "Node examples: [('R0', {'type': 'function_definition', 'name': 'root', 'docstring': '', 'start_line': 18, 'end_line': 19, 'file': 'main.py'}), ('R1', {'type': 'function_definition', 'name': 'get_token_header', 'docstring': 'Dependency to validate X-Token header', 'start_line': 5, 'end_line': 11, 'file': 'dependencies.py'}), ('R2', {'type': 'function_definition', 'name': 'get_query_token', 'docstring': 'Dependency to validate token query parameter', 'start_line': 14, 'end_line': 20, 'file': 'dependencies.py'})]\n"
     ]
    }
   ],
   "source": [
    "from networkx.classes.reportviews import NodeView, EdgeView\n",
    "print(\"Nodes:\", RG.number_of_nodes())\n",
    "print(\"Edges:\", RG.number_of_edges())\n",
    "print(\"Node examples:\", list(RG.nodes(data=True))[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e58aeb-2629-4e4d-b87f-492a07edb854",
   "metadata": {},
   "source": [
    "### 2.2 Add *parent_child* edges (calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ee355e-6932-452c-ad9b-6645d996bd76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent_child edges added. Graph now has 3 edges.\n"
     ]
    }
   ],
   "source": [
    "import ast, collections, itertools\n",
    "\n",
    "# 1. lookup (file, name)  ->  requirement ID\n",
    "name_to_rid = {\n",
    "    (data[\"file\"], data[\"name\"]): rid\n",
    "    for rid, data in RG.nodes(data=True)\n",
    "}\n",
    "\n",
    "def deepest_attr(node: ast.Attribute) -> str:\n",
    "    \"\"\"Return the last attribute name of a dotted call: pkg.mod.func -> func\"\"\"\n",
    "    while isinstance(node, ast.Attribute):\n",
    "        last = node.attr\n",
    "        node  = node.value\n",
    "    return last  # 'func'\n",
    "\n",
    "for py in ROOT.rglob(\"*.py\"):\n",
    "    src   = py.read_text(encoding=\"utf-8\")\n",
    "    mod   = ast.parse(src, filename=str(py))\n",
    "    file_ = str(py.relative_to(ROOT))\n",
    "\n",
    "    # todos los defs de este archivo\n",
    "    defs = {n.name: n for n in ast.walk(mod)\n",
    "            if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))}\n",
    "\n",
    "    for def_name, fn in defs.items():\n",
    "        rid_caller = name_to_rid.get((file_, def_name))\n",
    "        if not rid_caller:\n",
    "            continue\n",
    "\n",
    "        for call in ast.walk(fn):\n",
    "            if not isinstance(call, ast.Call):\n",
    "                continue\n",
    "\n",
    "            callee_name = None\n",
    "            # foo()\n",
    "            if isinstance(call.func, ast.Name):\n",
    "                callee_name = call.func.id\n",
    "            # obj.foo()  /  pkg.mod.bar()\n",
    "            elif isinstance(call.func, ast.Attribute):\n",
    "                callee_name = deepest_attr(call.func)\n",
    "\n",
    "            if callee_name:\n",
    "                rid_callee = name_to_rid.get((file_, callee_name))\n",
    "                if rid_callee:\n",
    "                    RG.add_edge(rid_caller, rid_callee, kind=\"parent_child\")\n",
    "\n",
    "print(f\"Parent_child edges added. Graph now has {RG.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c78199cd-f0fd-4507-96df-3dfa19e95bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = [\n",
    "    u for u, v, kind in RG.edges(data=\"kind\")\n",
    "    if kind == \"parent_child\"\n",
    "][:10]\n",
    "\n",
    "for rid in parents:\n",
    "    data = RG.nodes[rid]\n",
    "    print(f\"{rid}  {data['file']}:{data['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24452ece-a2d8-40f6-bafa-41070f850074",
   "metadata": {},
   "source": [
    "### Quick graph visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6692e3f0-0a61-4c06-a3c7-563d99d02c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "requirement_graph.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750px\"\n",
       "            src=\"requirement_graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x31327c8c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(height=\"750px\", width=\"100%\", notebook=True, directed=False)\n",
    "net.toggle_physics(True)\n",
    "\n",
    "for n, data in RG.nodes(data=True):\n",
    "    net.add_node(n, label=data[\"name\"], title=data[\"docstring\"][:200])\n",
    "\n",
    "for u, v, k in RG.edges(data=\"kind\"):\n",
    "    color = \"#2ca02c\" if k == \"parent_child\" else \"#9467bd\"\n",
    "    net.add_edge(u, v, color=color)\n",
    "\n",
    "net.show(\"requirement_graph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4bf533-95c9-4580-b915-20c739a29e40",
   "metadata": {},
   "source": [
    "## 3. DS-Code Graph (CG) – structure of the real code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a965e3b3-0236-46d7-977d-5125707ca4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CG built: 21 nodes  |  25 edges\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('main.py', 'C0', 'contain'),\n",
       " ('main.py', 'fastapi', 'import'),\n",
       " ('main.py', 'dependencies', 'import'),\n",
       " ('main.py', 'routers', 'import'),\n",
       " ('dependencies.py', 'C1', 'contain'),\n",
       " ('dependencies.py', 'C2', 'contain'),\n",
       " ('dependencies.py', 'typing', 'import'),\n",
       " ('dependencies.py', 'fastapi', 'import'),\n",
       " ('routers/tasks.py', 'C3', 'contain'),\n",
       " ('routers/tasks.py', 'C4', 'contain')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast, networkx as nx\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "ROOT = Path(\"./app\").resolve()\n",
    "\n",
    "# ---------- load elements ----------\n",
    "with open(\"code_elements_with_docstrings.json\") as f:\n",
    "    elements = json.load(f)\n",
    "\n",
    "# helper: (file, name) -> code-node id\n",
    "cid_map = {}\n",
    "CG = nx.DiGraph()\n",
    "\n",
    "# ---------- add code nodes ----------\n",
    "for idx, el in enumerate(elements):\n",
    "    cid = f\"C{idx}\"\n",
    "    cid_map[(el[\"file\"], el[\"name\"])] = cid\n",
    "    CG.add_node(cid, **el)\n",
    "\n",
    "# ---------- add file nodes ----------\n",
    "for el in elements:\n",
    "    CG.add_node(el[\"file\"], type=\"module\")\n",
    "\n",
    "# ---------- contain edges ----------\n",
    "for idx, el in enumerate(elements):\n",
    "    CG.add_edge(el[\"file\"], f\"C{idx}\", kind=\"contain\")\n",
    "\n",
    "# ---------- scan every file with ast ----------\n",
    "for py in ROOT.rglob(\"*.py\"):\n",
    "    file_id = str(py.relative_to(ROOT))\n",
    "    src     = py.read_text(encoding=\"utf-8\")\n",
    "    tree    = ast.parse(src, filename=file_id)\n",
    "\n",
    "    # --- import edges (file -> imported module/file) ---\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            for n in node.names:\n",
    "                CG.add_edge(file_id, n.name, kind=\"import\")  # crude, module string\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            mod = node.module or \"\"\n",
    "            CG.add_edge(file_id, mod, kind=\"import\")\n",
    "\n",
    "    # --- call + inherit edges inside this file ---\n",
    "    defs = {n.name: n for n in ast.walk(tree)\n",
    "            if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef))}\n",
    "\n",
    "    for def_name, obj in defs.items():\n",
    "        caller_cid = cid_map.get((file_id, def_name))\n",
    "        if not caller_cid:\n",
    "            continue\n",
    "\n",
    "        # inherit (classes only)\n",
    "        if isinstance(obj, ast.ClassDef):\n",
    "            for base in obj.bases:\n",
    "                if isinstance(base, ast.Name):\n",
    "                    parent_cid = cid_map.get((file_id, base.id))\n",
    "                    if parent_cid:\n",
    "                        CG.add_edge(caller_cid, parent_cid, kind=\"inherit\")\n",
    "\n",
    "        # call edges\n",
    "        for call in ast.walk(obj):\n",
    "            if isinstance(call, ast.Call):\n",
    "                # simple cases: foo(), obj.foo()\n",
    "                target_name = None\n",
    "                if isinstance(call.func, ast.Name):\n",
    "                    target_name = call.func.id\n",
    "                elif isinstance(call.func, ast.Attribute):\n",
    "                    target_name = call.func.attr\n",
    "                if target_name:\n",
    "                    callee_cid = cid_map.get((file_id, target_name))\n",
    "                    if callee_cid:\n",
    "                        CG.add_edge(caller_cid, callee_cid, kind=\"call\")\n",
    "\n",
    "print(f\"CG built: {CG.number_of_nodes()} nodes  |  {CG.number_of_edges()} edges\")\n",
    "\n",
    "# preview a few edges\n",
    "list(CG.edges(data=\"kind\"))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ee574-a347-4fbc-aa10-0080913a3015",
   "metadata": {},
   "source": [
    "### 3.1 Quick sanity-checks for the Code Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a939b8-0f46-41cc-ba44-8049a0676f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 21\n",
      "Edges: 25\n",
      "Edge types: Counter({'import': 13, 'contain': 12})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Nodes:\", CG.number_of_nodes())\n",
    "print(\"Edges:\", CG.number_of_edges())\n",
    "\n",
    "# distribución por tipo de arista\n",
    "edge_kinds = Counter(k for _,_,k in CG.edges(data=\"kind\"))\n",
    "print(\"Edge types:\", edge_kinds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e503f7-3a06-422e-9fec-264d53ec27f9",
   "metadata": {},
   "source": [
    "#### 3.2  Add `similar_to` edges inside the Code Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f579034f-3de2-4b95-845e-ed9206601672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➕ Added 4 code 'similar_to' edges – CG now has 29 edges\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "THRESH_CODE_SIM = 0.80          # similarity threshold\n",
    "EMBED_MODEL      = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "\n",
    "# 1️⃣ gather code-node ids (functions & classes only)\n",
    "code_nodes = [\n",
    "    n for n, d in CG.nodes(data=True)\n",
    "    if d.get(\"type\") in (\"function_definition\", \"class_definition\")\n",
    "]\n",
    "\n",
    "# 2️⃣ extract raw source text for each node\n",
    "def get_source(node_data):\n",
    "    file_path = Path(\"./app\") / node_data[\"file\"]\n",
    "    lines = file_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    return \"\\n\".join(lines[node_data[\"start_line\"]-1 : node_data[\"end_line\"]])\n",
    "\n",
    "corpus = [get_source(CG.nodes[n]) for n in code_nodes]\n",
    "\n",
    "# 3️⃣ embed & normalise\n",
    "model_code = SentenceTransformer(EMBED_MODEL)\n",
    "embeddings = model_code.encode(corpus, normalize_embeddings=True)\n",
    "\n",
    "# 4️⃣ compute cosine matrix (small n, so brute force is fine)\n",
    "cos_mat = cosine_similarity(embeddings)\n",
    "\n",
    "# 5️⃣ add edges\n",
    "added = 0\n",
    "for i in range(len(code_nodes)):\n",
    "    for j in range(i+1, len(code_nodes)):\n",
    "        if cos_mat[i, j] >= THRESH_CODE_SIM:\n",
    "            CG.add_edge(code_nodes[i], code_nodes[j],\n",
    "                        kind=\"similar_to\",\n",
    "                        weight=float(cos_mat[i, j]))\n",
    "            added += 1\n",
    "\n",
    "print(f\"➕ Added {added} code 'similar_to' edges – CG now has {CG.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ed0a1c2-50d9-417e-a8e9-51c14b4c7d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_token_header  ≃  get_query_token   (cos=0.80)\n",
      "Task  ≃  Agent   (cos=0.92)\n",
      "get_task  ≃  get_agent_tasks   (cos=0.87)\n",
      "get_agent_tasks  ≃  get_agent   (cos=0.85)\n"
     ]
    }
   ],
   "source": [
    "# show a few similarity pairs\n",
    "examples = [\n",
    "    (u, v, CG.edges[u, v][\"weight\"])\n",
    "    for u, v, k in CG.edges(data=\"kind\") if k == \"similar_to\"\n",
    "][:5]\n",
    "\n",
    "for u, v, w in examples:\n",
    "    print(f\"{CG.nodes[u]['name']}  ≃  {CG.nodes[v]['name']}   (cos={w:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed70fc-fb4d-4aba-9691-a484acf9dfa8",
   "metadata": {},
   "source": [
    "### 4. ID map  (Requirement  ↔  Code)  –  the Bigraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64bf02ff-d5b7-4ee4-949c-86fd4c221ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigraph mapping added: 12 links\n"
     ]
    }
   ],
   "source": [
    "# 1-to-1 map\n",
    "id_map = {f\"R{i}\": f\"C{i}\" for i in range(len(elements))}\n",
    "\n",
    "IDMAP = id_map  \n",
    "\n",
    "# guardamos como atributo cruzado\n",
    "for rid, cid in id_map.items():\n",
    "    RG.nodes[rid][\"code_id\"] = cid\n",
    "    CG.nodes[cid][\"req_id\"]  = rid\n",
    "print(\"Bigraph mapping added:\", len(id_map), \"links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a2dbda-10e4-469e-9e4a-7ac27eab0bb0",
   "metadata": {},
   "source": [
    "### 💾 Persist graphs to disk\n",
    "\n",
    "We’ll save both graphs in **GraphML** format so they can be:\n",
    "\n",
    "* Reloaded later in NetworkX without rebuilding.\n",
    "* Imported into Neo4j (via `neo4j-admin import`) or visual tools like Gephi.\n",
    "\n",
    "Files created:\n",
    "\n",
    "* `requirement_graph.graphml`\n",
    "* `code_graph.graphml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0899451b-0152-4de6-b66c-93a140fdcabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ graphs.npz escrito 🗂️\n"
     ]
    }
   ],
   "source": [
    "# ⬇️ EJECUTA ESTO DESPUÉS de construir RG, CG e id_map\n",
    "import numpy as np, pickle, networkx as nx\n",
    "\n",
    "# 1. Backup en formato NetworkX (GraphML) — útil para Neo4j, Gephi, etc.\n",
    "nx.write_graphml(RG, \"requirement_graph.graphml\")\n",
    "nx.write_graphml(CG, \"code_graph.graphml\")\n",
    "\n",
    "# 2. Backup “todo-en-uno” con NumPy (rápido para recargar en Jupyter)\n",
    "np.savez(\"graphs.npz\", RG=RG, CG=CG, id_map=id_map)\n",
    "print(\"✅ graphs.npz escrito 🗂️\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3cdf79-8179-44d2-a461-41f6d8df3b4e",
   "metadata": {},
   "source": [
    "# 5. CodeRAG Agent (local, ReAct style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a81f41-4513-4fcd-a29f-4c2df76e401e",
   "metadata": {},
   "source": [
    "## 5.1 Tools  (GraphReason · WebSearch · CodeTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a6f5291-c911-4874-91af-db80772c577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Dependencies\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "\n",
    "# ✅ Load .env\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Init LLM using the env key\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5721c33d-bd72-49ed-ab0f-d1f445b32a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Tools (GraphReason, WebSearch, CodeTest)\n",
    "from langchain.agents import Tool\n",
    "from pathlib import Path\n",
    "import pathlib, subprocess, tempfile, black\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Assume RG is already defined (networkx DiGraph or MultiDiGraph)\n",
    "def graph_reason(rid:str, n_sim=3, n_child=3):\n",
    "    sims, childs = [], []\n",
    "    for _, v, k in RG.edges(rid, data=\"kind\"):\n",
    "        if k==\"similar_to\" and len(sims) < n_sim: sims.append(RG.nodes[v])\n",
    "        if k==\"parent_child\" and len(childs) < n_child: childs.append(RG.nodes[v])\n",
    "    return {\"similar\": sims, \"child\": childs}\n",
    "\n",
    "def web_search(q:str, k=3):\n",
    "    with DDGS() as ddgs:\n",
    "        results = [r for r in ddgs.text(q, max_results=k)]\n",
    "    return \"\\n\".join(f\"{r['title']}: {r['body']}\" for r in results if 'body' in r)\n",
    "\n",
    "def code_test(code:str):\n",
    "    tmp = pathlib.Path(tempfile.mkstemp(suffix=\".py\")[1])\n",
    "    tmp.write_text(code, encoding=\"utf-8\")\n",
    "    black.format_file_in_place(tmp, fast=True, mode=black.FileMode())\n",
    "    proc = subprocess.run([\"python\", \"-m\", \"py_compile\", str(tmp)], capture_output=True, text=True)\n",
    "    return \"\\u2705 compiled ok\" if proc.returncode == 0 else proc.stderr[:300]\n",
    "\n",
    "def show_source(node):\n",
    "    file = node.get(\"file\")\n",
    "    start = node.get(\"start_line\")\n",
    "    end = node.get(\"end_line\")\n",
    "    if not file or not start or not end:\n",
    "        return \"File or line info missing.\"\n",
    "    lines = Path(file).read_text(encoding=\"utf-8\").splitlines()\n",
    "    return \"\\n\".join(lines[start-1:end])\n",
    "    \n",
    "def code_graph_lookup(func_name: str):\n",
    "    # Busca en los nodos del grafo de código\n",
    "    matches = [v for _, v, d in CG.edges(data=True)\n",
    "               if CG.nodes[v].get(\"name\") == func_name]\n",
    "\n",
    "    if not matches:\n",
    "        return f\"No match found for function `{func_name}`.\"\n",
    "\n",
    "    # Asume que es único (o devuelve el primero)\n",
    "    node_id = matches[0]\n",
    "    node_data = CG.nodes[node_id]\n",
    "\n",
    "    # Encuentra llamadas entrantes y salientes\n",
    "    calls = {\n",
    "        \"calls\": [CG.nodes[v] for _, v in CG.out_edges(node_id)],\n",
    "        \"called_by\": [CG.nodes[u] for u, _ in CG.in_edges(node_id)],\n",
    "    }\n",
    "\n",
    "    # Empaqueta respuesta\n",
    "    return {\n",
    "        \"function\": node_data,\n",
    "        **calls\n",
    "    }\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"LookupInGraph\", func=graph_reason, description=\"Input: Requirement id (e.g. R42) -> similar and child requirements\"),\n",
    "    Tool(name=\"WebSearch\", func=web_search, description=\"Input: string query -> DuckDuckGo search result titles and snippets\"),\n",
    "    Tool(name=\"CodeTest\", func=code_test, description=\"Input: Python code string -> formats with black, then compiles to check for syntax errors\")\n",
    "]\n",
    "\n",
    "tools.append(\n",
    "    Tool(\n",
    "        name=\"CodeGraphLookup\",\n",
    "        func=code_graph_lookup,\n",
    "        description=\"Input: function name (e.g. 'process_data') -> gets the function's docstring, file, and connected calls in the Code Graph.\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373f9e0-ec50-4196-bd61-02b7c0963820",
   "metadata": {},
   "source": [
    "## 5.2 Build the ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46ed1ac3-c3df-4690-9b52-6106f52f14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are **CodeRAG-Agent**, an expert developer assistant designed to help understand and reason about a codebase and its requirements.\n",
    "\n",
    "You have access to two graphs:\n",
    "1. **Requirement Graph (RG)** – a graph where each node is a software requirement (e.g. R0, R1, R42), connected by semantic relationships such as:\n",
    "   - `similar_to`: nodes that express related or overlapping functionality.\n",
    "   - `parent_child`: hierarchical breakdown of features or sub-requirements.\n",
    "\n",
    "2. **Code Graph (CG)** – a graph where each node is a function or class in the codebase (e.g. C0, C1). Nodes include:\n",
    "   - Metadata: name, type, file path, line range, docstring.\n",
    "   - Edges represent `calls` and `called_by` relationships between functions/classes.\n",
    "\n",
    "---\n",
    "\n",
    "Your job is to help the user understand:\n",
    "- What a requirement means and which code is relevant to it.\n",
    "- What a function or class does, how it connects to others, and its implementation details.\n",
    "- How different requirements or code elements relate structurally or semantically.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Tools at your disposal\n",
    "\n",
    "1. **LookupInGraph**\n",
    "   - Input: requirement id (e.g. \"R42\").\n",
    "   - Use this to explore `similar` and `child` requirements.\n",
    "   - Always call this FIRST when a requirement is mentioned.\n",
    "\n",
    "2. **CodeGraphLookup**\n",
    "   - Input: function or class name (e.g. \"process_data\").\n",
    "   - Use this to retrieve metadata from the Code Graph (CG), including file, lines, docstring, and related functions (calls/called_by).\n",
    "   - Use this for all code-related questions.\n",
    "\n",
    "3. **WebSearch**\n",
    "   - Input: any query string.\n",
    "   - Use only if the requirement or function lacks context in the graphs.\n",
    "\n",
    "4. **CodeTest**\n",
    "   - Input: Python code string.\n",
    "   - Use this to auto-format the code and test it for syntax errors (compilation via `black` and `py_compile`).\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Strategy for every query\n",
    "\n",
    "- If the user mentions a requirement (e.g. \"R42\"), always call **LookupInGraph** first.\n",
    "- If the user asks about a class or function, use **CodeGraphLookup**.\n",
    "- Extract context from the graphs before answering.\n",
    "- If context is still insufficient, optionally use **WebSearch** to enrich your answer.\n",
    "- If you produce or edit Python code, test it with **CodeTest** before showing the final version.\n",
    "- Always return the final answer as clear text, optionally including relevant code snippets or summaries.\n",
    "\n",
    "Do not make assumptions. Always ground your answers in the graph context or tool outputs.\n",
    "If a tool fails or the input isn't found, explain what you tried and ask the user for clarification if needed.\n",
    "\"\"\"\n",
    "\n",
    "# ✅ LangGraph checkpointing (in-memory)\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# ✅ Create the agent\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    "    checkpointer=checkpointer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c91c97-a9e6-46a9-9ebc-b62612800049",
   "metadata": {},
   "source": [
    "## 4.3 Ask the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ae15231-d15b-4943-a975-6369a09dffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_agent_response(response):\n",
    "    messages = response.get(\"messages\", [])\n",
    "    for msg in messages:\n",
    "        role = type(msg).__name__.replace(\"Message\", \"\")\n",
    "        content = getattr(msg, \"content\", \"\")\n",
    "        name = getattr(msg, \"name\", None)\n",
    "\n",
    "        if name:\n",
    "            print(f\"🛠️ Tool ({name}):\\n{content}\\n\")\n",
    "        else:\n",
    "            prefix = \"🤖 AI\" if role == \"AI\" else \"🧑 Human\"\n",
    "            print(f\"{prefix}:\\n{content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "905df1e4-cec4-4486-aed7-7a594c74955d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑 Human:\n",
      "how i can us the function get agent\n",
      "\n",
      "🤖 AI:\n",
      "\n",
      "\n",
      "🛠️ Tool (CodeGraphLookup):\n",
      "{\"function\": {\"type\": \"function_definition\", \"name\": \"get_agent\", \"docstring\": \"Get a specific agent by ID\", \"start_line\": 40, \"end_line\": 47, \"file\": \"routers/agents.py\", \"req_id\": \"R10\"}, \"calls\": [], \"called_by\": [{\"type\": \"module\"}, {\"type\": \"function_definition\", \"name\": \"get_agent_tasks\", \"docstring\": \"Get all tasks assigned to a specific agent\", \"start_line\": 54, \"end_line\": 61, \"file\": \"routers/tasks.py\", \"req_id\": \"R6\"}]}\n",
      "\n",
      "🤖 AI:\n",
      "The function `get_agent` is defined in the file `routers/agents.py`, from lines 40 to 47. Its purpose is to \"Get a specific agent by ID\". \n",
      "\n",
      "### Usage\n",
      "To use the `get_agent` function, you would typically call it with the ID of the agent you want to retrieve. However, the specific parameters and return values are not detailed in the docstring, so you would need to look at the function's implementation in the file for more details.\n",
      "\n",
      "### Connections\n",
      "- **Called By**: The `get_agent` function is called by another function named `get_agent_tasks` in the file `routers/tasks.py` (lines 54 to 61). This function is responsible for getting all tasks assigned to a specific agent.\n",
      "\n",
      "If you need more detailed information about the parameters or the implementation, you should check the code in the specified file and lines. If you need help with that, let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"how i can us the function get agent\"}]},\n",
    "    config={\"configurable\": {\"thread_id\": \"session-001\"}}\n",
    ")\n",
    "\n",
    "display_agent_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
